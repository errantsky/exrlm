<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>RLM Engine — Design Guide</title>
  <style>
    :root {
      --bg: #0f1117;
      --surface: #1a1d27;
      --surface-2: #252834;
      --border: #2e3140;
      --text: #e1e4ed;
      --text-dim: #8b8fa3;
      --accent: #7c6cf0;
      --accent-light: #9d8ff7;
      --green: #4ade80;
      --amber: #fbbf24;
      --red: #f87171;
      --blue: #60a5fa;
      --cyan: #22d3ee;
      --mono: 'SF Mono', 'Fira Code', 'JetBrains Mono', 'Consolas', monospace;
      --sans: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }

    body {
      font-family: var(--sans);
      background: var(--bg);
      color: var(--text);
      line-height: 1.7;
      font-size: 15px;
    }

    .container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 2rem;
    }

    /* Header */
    header {
      text-align: center;
      padding: 4rem 2rem 3rem;
      border-bottom: 1px solid var(--border);
    }

    header h1 {
      font-size: 2.5rem;
      font-weight: 700;
      background: linear-gradient(135deg, var(--accent-light), var(--cyan));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      margin-bottom: 0.5rem;
    }

    header .subtitle {
      color: var(--text-dim);
      font-size: 1.1rem;
      max-width: 600px;
      margin: 0 auto;
    }

    header .badges {
      margin-top: 1.5rem;
      display: flex;
      gap: 0.75rem;
      justify-content: center;
      flex-wrap: wrap;
    }

    .badge {
      display: inline-block;
      padding: 0.25rem 0.75rem;
      border-radius: 9999px;
      font-size: 0.8rem;
      font-weight: 600;
      border: 1px solid var(--border);
      background: var(--surface);
    }

    .badge.elixir { border-color: #9b59b6; color: #c39bd3; }
    .badge.otp { border-color: #e74c3c; color: #f1948a; }
    .badge.claude { border-color: var(--accent); color: var(--accent-light); }

    /* Sections */
    section {
      padding: 3rem 0;
      border-bottom: 1px solid var(--border);
    }

    section:last-child { border-bottom: none; }

    h2 {
      font-size: 1.6rem;
      font-weight: 700;
      margin-bottom: 1.5rem;
      color: var(--text);
    }

    h2 .num {
      color: var(--accent);
      font-family: var(--mono);
      font-size: 0.9rem;
      margin-right: 0.5rem;
    }

    h3 {
      font-size: 1.15rem;
      font-weight: 600;
      margin: 1.5rem 0 0.75rem;
      color: var(--text);
    }

    p { margin-bottom: 1rem; color: var(--text-dim); }

    /* Cards */
    .card-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 1rem;
    }

    .card {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1.5rem;
      transition: border-color 0.2s;
    }

    .card:hover { border-color: var(--accent); }

    .card h4 {
      font-family: var(--mono);
      font-size: 0.95rem;
      color: var(--accent-light);
      margin-bottom: 0.5rem;
    }

    .card p { font-size: 0.9rem; margin-bottom: 0; }

    .card .tag {
      display: inline-block;
      margin-top: 0.75rem;
      padding: 0.15rem 0.5rem;
      border-radius: 4px;
      font-size: 0.75rem;
      font-family: var(--mono);
      background: var(--surface-2);
      color: var(--text-dim);
    }

    /* Diagrams (ASCII art) */
    .diagram {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1.5rem;
      overflow-x: auto;
      margin: 1rem 0;
    }

    .diagram pre {
      font-family: var(--mono);
      font-size: 0.82rem;
      line-height: 1.5;
      color: var(--text);
      white-space: pre;
    }

    /* Code blocks */
    pre.code {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 1.25rem;
      overflow-x: auto;
      margin: 1rem 0;
      font-family: var(--mono);
      font-size: 0.85rem;
      line-height: 1.6;
      color: var(--text);
    }

    .kw { color: var(--accent-light); }
    .fn { color: var(--cyan); }
    .str { color: var(--green); }
    .cmt { color: var(--text-dim); font-style: italic; }
    .num-lit { color: var(--amber); }
    .atom { color: var(--blue); }

    /* Invariants */
    .invariant-list {
      list-style: none;
      display: grid;
      gap: 1rem;
      margin: 1rem 0;
    }

    .invariant-list li {
      background: var(--surface);
      border-left: 3px solid var(--accent);
      border-radius: 0 8px 8px 0;
      padding: 1rem 1.25rem;
    }

    .invariant-list li strong {
      display: block;
      color: var(--text);
      margin-bottom: 0.25rem;
    }

    .invariant-list li span {
      color: var(--text-dim);
      font-size: 0.9rem;
    }

    /* Table */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
      font-size: 0.9rem;
    }

    th {
      text-align: left;
      padding: 0.75rem 1rem;
      background: var(--surface);
      border-bottom: 2px solid var(--accent);
      color: var(--text);
      font-weight: 600;
    }

    td {
      padding: 0.6rem 1rem;
      border-bottom: 1px solid var(--border);
      color: var(--text-dim);
    }

    td:first-child {
      font-family: var(--mono);
      color: var(--accent-light);
      font-size: 0.85rem;
      white-space: nowrap;
    }

    /* Telemetry events */
    .event-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 0.5rem;
      margin: 1rem 0;
    }

    .event-item {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 0.6rem 0.9rem;
      font-family: var(--mono);
      font-size: 0.78rem;
      color: var(--cyan);
    }

    /* Flow steps */
    .flow-steps {
      position: relative;
      padding-left: 2rem;
      margin: 1rem 0;
    }

    .flow-steps::before {
      content: '';
      position: absolute;
      left: 0.6rem;
      top: 0;
      bottom: 0;
      width: 2px;
      background: var(--border);
    }

    .flow-step {
      position: relative;
      padding: 0.75rem 0;
    }

    .flow-step::before {
      content: '';
      position: absolute;
      left: -1.65rem;
      top: 1.1rem;
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: var(--accent);
      border: 2px solid var(--bg);
    }

    .flow-step .label {
      font-weight: 600;
      color: var(--text);
      font-size: 0.95rem;
    }

    .flow-step .detail {
      color: var(--text-dim);
      font-size: 0.88rem;
      margin-top: 0.2rem;
    }

    .flow-step code {
      font-family: var(--mono);
      font-size: 0.82rem;
      background: var(--surface-2);
      padding: 0.1rem 0.4rem;
      border-radius: 4px;
      color: var(--cyan);
    }

    /* Config table */
    .config-table td:nth-child(2) {
      font-family: var(--mono);
      color: var(--green);
      font-size: 0.82rem;
    }

    /* Footer */
    footer {
      padding: 2rem 0;
      text-align: center;
      color: var(--text-dim);
      font-size: 0.85rem;
    }

    footer a {
      color: var(--accent-light);
      text-decoration: none;
    }

    /* Responsive */
    @media (max-width: 600px) {
      .container { padding: 1rem; }
      header h1 { font-size: 1.8rem; }
      .card-grid { grid-template-columns: 1fr; }
    }
  </style>
</head>
<body>

<header>
  <div class="container">
    <h1>RLM Engine</h1>
    <p class="subtitle">
      An OTP-native Recursive Language Model engine that lets an LLM write and execute
      Elixir code in a persistent REPL to process unbounded-length input.
    </p>
    <div class="badges">
      <span class="badge elixir">Elixir 1.19 / OTP 28</span>
      <span class="badge otp">OTP Supervision</span>
      <span class="badge claude">Claude API</span>
    </div>
  </div>
</header>

<div class="container">

  <!-- Section 1: Core Concept -->
  <section>
    <h2><span class="num">01</span> What is RLM?</h2>
    <p>
      The Recursive Language Model (RLM) is an inference strategy where an LLM never sees raw input data.
      Instead, it receives metadata (byte size, line count, preview) and writes Elixir code in a persistent
      REPL to process the data. The LLM can delegate sub-tasks to smaller LLMs, enabling recursive
      decomposition of arbitrarily large inputs.
    </p>
    <p>
      The engine enforces three invariants that keep the LLM's context window small regardless of
      input size:
    </p>
    <ul class="invariant-list">
      <li>
        <strong>Invariant 1 &mdash; Input Isolation</strong>
        <span>Raw input data is stored in the <code>context</code> binding. The LLM only sees metadata
        (byte size, line count) and a truncated preview. It must write code to process the actual data.</span>
      </li>
      <li>
        <strong>Invariant 2 &mdash; Sub-call Isolation</strong>
        <span>Results from sub-LLM calls (<code>lm_query</code>) are stored in variables within the REPL.
        The parent LLM never sees child outputs in its context &mdash; only the variable bindings.</span>
      </li>
      <li>
        <strong>Invariant 3 &mdash; Stdout Truncation</strong>
        <span>Stdout output from code execution is truncated using a head+tail strategy (configurable,
        default 4000 chars each). Important results must be stored in variables.</span>
      </li>
    </ul>
  </section>

  <!-- Section 2: Architecture -->
  <section>
    <h2><span class="num">02</span> Architecture Overview</h2>

    <h3>Supervision Tree</h3>
    <div class="diagram">
      <pre>
  RLM.Supervisor (one_for_one)
  |
  +-- Registry (RLM.Registry)          unique name registry for Workers + EventLogs
  |
  +-- Phoenix.PubSub (RLM.PubSub)     real-time event broadcasting
  |
  +-- DynamicSupervisor (RLM.WorkerSup)
  |   |
  |   +-- RLM.Worker (span_abc)        :temporary — dies after completion
  |   +-- RLM.Worker (span_def)        child spawned by subcall
  |   +-- ...
  |
  +-- DynamicSupervisor (RLM.EventStore)
  |   |
  |   +-- RLM.EventLog (run_xyz)       Agent storing reasoning trace
  |   +-- ...
  |
  +-- RLM.Telemetry (GenServer)        attaches all telemetry handlers on boot
      </pre>
    </div>
    <p>
      Workers are started under <code>RLM.WorkerSup</code> with <code>restart: :temporary</code> &mdash;
      they terminate normally after producing a result and are not restarted. EventLog Agents are
      per-run and hold the full execution trace for dashboard consumption.
    </p>

    <h3>Execution Flow</h3>
    <div class="flow-steps">
      <div class="flow-step">
        <div class="label">1. API Entry</div>
        <div class="detail"><code>RLM.run(context, query, opts)</code> loads config, generates span/run IDs, starts a Worker under DynamicSupervisor</div>
      </div>
      <div class="flow-step">
        <div class="label">2. Worker Init</div>
        <div class="detail">Worker stores <code>context</code> in bindings (never sent to LLM), builds system + user messages with metadata only, sends <code>:iterate</code> to self</div>
      </div>
      <div class="flow-step">
        <div class="label">3. LLM Call</div>
        <div class="detail">Worker calls <code>llm_module.chat(history, model, config)</code> synchronously via the Anthropic Messages API</div>
      </div>
      <div class="flow-step">
        <div class="label">4. Code Extraction</div>
        <div class="detail">Response is scanned for <code>```elixir</code> blocks. If none found, a nudge is appended and iteration continues</div>
      </div>
      <div class="flow-step">
        <div class="label">5. Async Eval</div>
        <div class="detail">Code is evaluated in a spawned process via <code>Code.eval_string</code> with IO capture. Worker remains free to handle subcall requests</div>
      </div>
      <div class="flow-step">
        <div class="label">6. Result Check</div>
        <div class="detail">If <code>final_answer</code> binding is non-nil, Worker completes and sends result to caller. Otherwise, stdout feedback is added to history and iteration continues</div>
      </div>
    </div>
  </section>

  <!-- Section 3: The Async Eval Pattern -->
  <section>
    <h2><span class="num">03</span> The Async Eval Pattern</h2>
    <p>
      The most critical architectural decision in the engine. Without it, sub-LLM calls would deadlock.
    </p>
    <div class="diagram">
      <pre>
  Worker (GenServer)                        Eval Process (spawned)
  ==================                        =====================
        |
        |  handle_info(:iterate)
        |  ---- call LLM (sync, OK) ------>
        |  ---- spawn eval process ------->  Code.eval_string(code, bindings)
        |  {:noreply, state}                      |
        |                                         |  lm_query("sub input")
        |  <--- GenServer.call(:spawn_subcall) ---|
        |  handle_call: spawn child Worker        |
        |  {:noreply, pending_subcalls + from}    |  (blocked, waiting for reply)
        |                                         |
        |  <--- {:rlm_result, child_id, val} ---  |  (child Worker finishes)
        |  GenServer.reply(from, val)             |
        |                                   ----->|  (unblocked, continues eval)
        |                                         |
        |  <--- {:eval_complete, result} ---------|  (eval done)
        |  handle_info: process result
        |  check final_answer
        |
      </pre>
    </div>
    <p>
      If eval ran synchronously inside <code>handle_info</code>, the Worker would be blocked and unable to
      process the <code>:spawn_subcall</code> GenServer call from the eval process &mdash; classic GenServer deadlock.
      The async pattern keeps the Worker's message loop active during evaluation.
    </p>
  </section>

  <!-- Section 4: Module Reference -->
  <section>
    <h2><span class="num">04</span> Module Reference</h2>

    <div class="card-grid">
      <div class="card">
        <h4>RLM</h4>
        <p>Public API. <code>run/3</code> (synchronous) and <code>run_async/3</code> start a Worker and block/return immediately.</p>
        <span class="tag">lib/rlm.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Worker</h4>
        <p>GenServer that owns one execution node. Runs the iterate loop: LLM call, code eval, result check. Handles subcall spawning for recursive decomposition.</p>
        <span class="tag">lib/rlm/worker.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Eval</h4>
        <p>Sandboxed code evaluation with IO capture, timeout, and process monitoring. Injects worker PID for sandbox functions.</p>
        <span class="tag">lib/rlm/eval.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Sandbox</h4>
        <p>Functions available inside eval'd code: <code>lm_query</code>, <code>parallel_query</code>, <code>chunks</code>, <code>grep</code>, <code>preview</code>, <code>list_bindings</code>.</p>
        <span class="tag">lib/rlm/sandbox.ex</span>
      </div>

      <div class="card">
        <h4>RLM.LLM</h4>
        <p>Anthropic Messages API client using Req. Extracts system messages, formats for Claude API, parses code blocks and token usage.</p>
        <span class="tag">lib/rlm/llm.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Config</h4>
        <p>Configuration struct with 20+ fields. Loads from app env with runtime overrides. Supports dependency injection via <code>llm_module</code>.</p>
        <span class="tag">lib/rlm/config.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Prompt</h4>
        <p>System prompt loading from <code>priv/system_prompt.md</code> with fallback. Builds user, feedback, nudge, and compaction messages.</p>
        <span class="tag">lib/rlm/prompt.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Helpers</h4>
        <p>Utility functions: lazy string chunking via Stream.unfold, regex/string grep, truncated preview, binding introspection.</p>
        <span class="tag">lib/rlm/helpers.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Truncate</h4>
        <p>Head+tail string truncation with configurable sizes and an omission marker showing character count.</p>
        <span class="tag">lib/rlm/truncate.ex</span>
      </div>

      <div class="card">
        <h4>RLM.EventLog</h4>
        <p>Per-run Agent that stores structured events and builds a tree representation. Exports to JSONL for analysis.</p>
        <span class="tag">lib/rlm/event_log.ex</span>
      </div>

      <div class="card">
        <h4>RLM.Telemetry</h4>
        <p>GenServer that attaches Logger, EventLog, and PubSub handlers to 14 telemetry event types on boot.</p>
        <span class="tag">lib/rlm/telemetry/</span>
      </div>

      <div class="card">
        <h4>RLM.Span</h4>
        <p>Generates cryptographically random hex span IDs (16 bytes) and run IDs (8 bytes) for distributed tracing.</p>
        <span class="tag">lib/rlm/span.ex</span>
      </div>
    </div>
  </section>

  <!-- Section 5: Sandbox Helpers -->
  <section>
    <h2><span class="num">05</span> Sandbox Helpers</h2>
    <p>These functions are auto-imported into every eval'd code block via <code>import RLM.Sandbox</code>:</p>

    <table>
      <tr>
        <th>Function</th>
        <th>Description</th>
      </tr>
      <tr>
        <td>lm_query(text, opts)</td>
        <td>Invoke a sub-LLM. Blocks until child Worker completes. Returns <code>{:ok, answer}</code> or <code>{:error, reason}</code>. Options: <code>model_size: :small | :large</code></td>
      </tr>
      <tr>
        <td>parallel_query(inputs, opts)</td>
        <td>Invoke multiple sub-LLMs concurrently via Task.async. Each input can be a string or <code>{text, opts}</code> tuple. Returns results in order.</td>
      </tr>
      <tr>
        <td>chunks(string, size)</td>
        <td>Lazily split a string into chunks of <code>size</code> characters using Stream.unfold. Memory-efficient for large inputs.</td>
      </tr>
      <tr>
        <td>grep(pattern, string)</td>
        <td>Search for lines matching a substring or Regex. Returns <code>[{line_number, line_text}]</code> tuples.</td>
      </tr>
      <tr>
        <td>preview(term, n \\ 500)</td>
        <td>Truncated <code>inspect</code> of any term. Useful for examining large data without flooding stdout.</td>
      </tr>
      <tr>
        <td>list_bindings()</td>
        <td>Returns <code>[{name, type, byte_size}]</code> for all current REPL bindings.</td>
      </tr>
    </table>
  </section>

  <!-- Section 6: Telemetry Events -->
  <section>
    <h2><span class="num">06</span> Telemetry Events</h2>
    <p>
      The engine emits fine-grained telemetry events via <code>:telemetry.execute/3</code>. Three consumers
      are attached by default:
    </p>
    <table>
      <tr><th>Handler</th><th>Purpose</th></tr>
      <tr><td>RLM.Telemetry.Logger</td><td>Structured logging via Elixir Logger</td></tr>
      <tr><td>RLM.Telemetry.EventLogHandler</td><td>Routes events to per-run EventLog Agent</td></tr>
      <tr><td>RLM.Telemetry.PubSub</td><td>Broadcasts via Phoenix.PubSub to <code>"rlm:runs"</code> and <code>"rlm:run:#{run_id}"</code></td></tr>
    </table>

    <h3>Event Names</h3>
    <div class="event-grid">
      <div class="event-item">[:rlm, :node, :start]</div>
      <div class="event-item">[:rlm, :node, :stop]</div>
      <div class="event-item">[:rlm, :node, :exception]</div>
      <div class="event-item">[:rlm, :iteration, :start]</div>
      <div class="event-item">[:rlm, :iteration, :stop]</div>
      <div class="event-item">[:rlm, :llm, :request, :start]</div>
      <div class="event-item">[:rlm, :llm, :request, :stop]</div>
      <div class="event-item">[:rlm, :llm, :request, :exception]</div>
      <div class="event-item">[:rlm, :eval, :start]</div>
      <div class="event-item">[:rlm, :eval, :stop]</div>
      <div class="event-item">[:rlm, :eval, :exception]</div>
      <div class="event-item">[:rlm, :subcall, :spawn]</div>
      <div class="event-item">[:rlm, :subcall, :result]</div>
      <div class="event-item">[:rlm, :compaction, :run]</div>
    </div>
    <p>
      Each event carries <code>measurements</code> (e.g., <code>duration_ms</code>, <code>total_tokens</code>) and
      <code>metadata</code> (e.g., <code>span_id</code>, <code>run_id</code>, <code>depth</code>, <code>model</code>).
      The iteration stop event is the richest, including code, stdout preview, eval status,
      token usage, and binding snapshots.
    </p>
  </section>

  <!-- Section 7: Configuration -->
  <section>
    <h2><span class="num">07</span> Configuration</h2>
    <p>
      Configuration is managed via <code>RLM.Config</code>. Defaults load from application env, and
      any field can be overridden at runtime via the <code>opts</code> keyword list passed to <code>RLM.run/3</code>.
    </p>

    <table class="config-table">
      <tr><th>Field</th><th>Default</th><th>Description</th></tr>
      <tr><td>api_base_url</td><td>"https://api.anthropic.com"</td><td>Anthropic API base URL</td></tr>
      <tr><td>api_key</td><td>$CLAUDE_API_KEY</td><td>API key from environment variable</td></tr>
      <tr><td>model_large</td><td>"claude-sonnet-4-5-..."</td><td>Model for root-level Workers</td></tr>
      <tr><td>model_small</td><td>"claude-haiku-4-5-..."</td><td>Model for sub-call Workers</td></tr>
      <tr><td>max_iterations</td><td>25</td><td>Max REPL iterations before forced stop</td></tr>
      <tr><td>max_depth</td><td>5</td><td>Max recursion depth for subcalls</td></tr>
      <tr><td>max_concurrent_subcalls</td><td>10</td><td>Max parallel subcalls per Worker</td></tr>
      <tr><td>truncation_head</td><td>4000</td><td>Chars to keep from start of stdout</td></tr>
      <tr><td>truncation_tail</td><td>4000</td><td>Chars to keep from end of stdout</td></tr>
      <tr><td>eval_timeout</td><td>300,000 ms</td><td>Per-eval execution timeout</td></tr>
      <tr><td>llm_timeout</td><td>120,000 ms</td><td>Per-LLM-call HTTP timeout</td></tr>
      <tr><td>llm_module</td><td>RLM.LLM</td><td>Swappable LLM client (for testing)</td></tr>
    </table>
  </section>

  <!-- Section 8: Safety Mechanisms -->
  <section>
    <h2><span class="num">08</span> Safety Mechanisms</h2>

    <div class="card-grid">
      <div class="card">
        <h4>Iteration Cap</h4>
        <p>Workers stop after <code>max_iterations</code> (default 25) to prevent runaway loops. Returns an error result.</p>
      </div>
      <div class="card">
        <h4>Depth Cap</h4>
        <p>Subcall depth is limited to <code>max_depth</code> (default 5). Prevents unbounded recursive decomposition.</p>
      </div>
      <div class="card">
        <h4>Eval Timeout</h4>
        <p>Each code evaluation has a 5-minute timeout. The eval process is killed with <code>Process.exit(pid, :kill)</code>.</p>
      </div>
      <div class="card">
        <h4>Repetition Detection</h4>
        <p>Jaccard similarity &gt; 0.85 across last 3 code blocks triggers a nudge message to break the LLM out of loops.</p>
      </div>
      <div class="card">
        <h4>History Compaction</h4>
        <p>When estimated tokens exceed 80% of context window, conversation history is serialized and compressed into a binding.</p>
      </div>
      <div class="card">
        <h4>Stdout Truncation</h4>
        <p>Head+tail truncation keeps stdout feedback bounded. Omitted middle section shows character count.</p>
      </div>
    </div>
  </section>

  <!-- Section 9: Usage Examples -->
  <section>
    <h2><span class="num">09</span> Usage Examples</h2>

    <h3>Basic Usage</h3>
    <pre class="code"><span class="cmt"># Simple query — count words in a string</span>
{<span class="atom">:ok</span>, answer} = <span class="fn">RLM.run</span>(
  <span class="str">"The quick brown fox jumps over the lazy dog"</span>,
  <span class="str">"Count the number of words. Set final_answer to the integer count."</span>
)

<span class="cmt"># answer will be 9</span></pre>

    <h3>With Config Overrides</h3>
    <pre class="code"><span class="cmt"># Use Haiku for both models, limit iterations</span>
{<span class="atom">:ok</span>, answer} = <span class="fn">RLM.run</span>(
  large_document,
  <span class="str">"Summarize the key findings"</span>,
  <span class="atom">model_large:</span> <span class="str">"claude-haiku-4-5-20251001"</span>,
  <span class="atom">model_small:</span> <span class="str">"claude-haiku-4-5-20251001"</span>,
  <span class="atom">max_iterations:</span> <span class="num-lit">10</span>
)</pre>

    <h3>Async Execution</h3>
    <pre class="code"><span class="cmt"># Start execution, get run_id for tracking</span>
{<span class="atom">:ok</span>, run_id, _pid} = <span class="fn">RLM.run_async</span>(context, query)

<span class="cmt"># Subscribe to real-time updates</span>
<span class="fn">Phoenix.PubSub.subscribe</span>(RLM.PubSub, <span class="str">"rlm:run:</span>#{run_id}<span class="str">"</span>)

<span class="cmt"># Receive telemetry events as messages</span>
<span class="kw">receive do</span>
  {<span class="atom">:rlm_event</span>, event} -> <span class="fn">IO.inspect</span>(event)
<span class="kw">end</span></pre>

    <h3>Inspecting Execution Traces</h3>
    <pre class="code"><span class="cmt"># After a run completes, inspect the reasoning trace</span>
tree = <span class="fn">RLM.EventLog.get_tree</span>(run_id)

<span class="cmt"># Export as JSONL for analysis</span>
jsonl = <span class="fn">RLM.EventLog.to_jsonl</span>(run_id)
<span class="fn">File.write!</span>(<span class="str">"trace.jsonl"</span>, jsonl)</pre>
  </section>

  <!-- Section 10: Testing -->
  <section>
    <h2><span class="num">10</span> Testing</h2>
    <p>
      Tests use <code>RLM.Test.MockLLM</code>, a mock that implements the LLM behaviour with
      a global ETS-based FIFO response queue. This enables deterministic testing of the
      full execution pipeline without API calls.
    </p>

    <pre class="code"><span class="cmt"># Program mock responses</span>
<span class="fn">RLM.Test.MockLLM.program_responses</span>(<span class="fn">self</span>(), [
  <span class="str">"```elixir\nresult = String.length(context)\nfinal_answer = result\n```"</span>
])

<span class="cmt"># Run with mock</span>
<span class="kw">assert</span> {<span class="atom">:ok</span>, <span class="num-lit">11</span>} = <span class="fn">RLM.run</span>(<span class="str">"hello world"</span>, <span class="str">"count chars"</span>,
  <span class="atom">llm_module:</span> RLM.Test.MockLLM
)</pre>

    <p>
      The test suite includes 37 tests covering: single/multi-iteration runs, binding persistence,
      error recovery, subcall spawning, telemetry emission, event log recording, JSONL export,
      sandbox helpers, and configuration loading. A separate live API test (tagged <code>:live_api</code>)
      calls the real Claude API.
    </p>

    <pre class="code"><span class="cmt"># Run all mock tests</span>
$ mix test

<span class="cmt"># Run with verbose output</span>
$ mix test --trace

<span class="cmt"># Include live API tests (requires CLAUDE_API_KEY)</span>
$ mix test --include live_api</pre>
  </section>

  <!-- Section 11: Future Phases -->
  <section>
    <h2><span class="num">11</span> Future Phases</h2>
    <p>The current implementation covers Phase 1 (Core Engine) and Phase 2 (Telemetry &amp; Event Log). The design spec outlines two additional phases:</p>

    <div class="card-grid">
      <div class="card">
        <h4>Phase 3: Session &amp; CLI</h4>
        <p>Named sessions with persistence, a Mix task CLI (<code>mix rlm.run</code>), and configurable retry/backoff for LLM calls.</p>
        <span class="tag">planned</span>
      </div>
      <div class="card">
        <h4>Phase 4: Phoenix Dashboard</h4>
        <p>Real-time LiveView dashboard showing execution trees, iteration details, token usage, and cost tracking. Consumes PubSub events.</p>
        <span class="tag">planned</span>
      </div>
    </div>
  </section>

</div>

<footer>
  <div class="container">
    <p>RLM Engine &mdash; Built with Elixir &amp; OTP &mdash; Powered by Claude</p>
  </div>
</footer>

</body>
</html>
