<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RLM Elixir Agent &mdash; Architecture Review</title>
<style>
  :root {
    --bg: #0d1117; --fg: #c9d1d9; --accent: #58a6ff; --accent2: #7ee787;
    --warn: #d29922; --err: #f85149; --card: #161b22; --border: #30363d;
    --code-bg: #1c2128; --heading: #f0f6fc;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
         background: var(--bg); color: var(--fg); line-height: 1.7; padding: 2rem; max-width: 1100px; margin: 0 auto; }
  h1 { color: var(--heading); font-size: 2rem; margin-bottom: .5rem; border-bottom: 2px solid var(--accent); padding-bottom: .5rem; }
  h2 { color: var(--accent); font-size: 1.5rem; margin: 2.5rem 0 1rem; }
  h3 { color: var(--accent2); font-size: 1.15rem; margin: 1.5rem 0 .5rem; }
  p, li { margin-bottom: .6rem; }
  ul, ol { padding-left: 1.5rem; }
  a { color: var(--accent); text-decoration: none; }
  a:hover { text-decoration: underline; }
  code { background: var(--code-bg); padding: 2px 6px; border-radius: 4px; font-size: .9em; }
  pre { background: var(--code-bg); padding: 1rem; border-radius: 8px; overflow-x: auto; margin: 1rem 0;
        border: 1px solid var(--border); font-size: .85em; line-height: 1.5; }
  .card { background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem; margin: 1rem 0; }
  .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; }
  @media (max-width: 700px) { .grid { grid-template-columns: 1fr; } }
  .badge { display: inline-block; padding: 2px 10px; border-radius: 12px; font-size: .8em; font-weight: 600; }
  .good { background: #1a3a2a; color: var(--accent2); }
  .warn { background: #3a2a1a; color: var(--warn); }
  .err  { background: #3a1a1a; color: var(--err); }
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
  th, td { text-align: left; padding: .5rem .75rem; border-bottom: 1px solid var(--border); }
  th { color: var(--accent); font-size: .85em; text-transform: uppercase; letter-spacing: .05em; }
  .toc { background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem; margin: 1.5rem 0; }
  .toc ol { counter-reset: toc; list-style: none; padding-left: 0; }
  .toc li { counter-increment: toc; margin-bottom: .3rem; }
  .toc li::before { content: counter(toc) ". "; color: var(--accent); font-weight: 600; }
  .diagram { background: var(--code-bg); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem; margin: 1rem 0;
             font-family: monospace; white-space: pre; line-height: 1.4; font-size: .85em; overflow-x: auto; }
  .subtitle { color: #8b949e; font-size: 1rem; margin-bottom: 2rem; }
  hr { border: none; border-top: 1px solid var(--border); margin: 3rem 0; }
</style>
</head>
<body>

<h1>RLM Elixir Agent &mdash; Architecture Review</h1>
<p class="subtitle">A comprehensive analysis of the current codebase, the original RLM (Python), and the Pi coding agent &mdash; with recommendations for the canonical OTP-native coding agent.</p>

<nav class="toc">
<strong>Contents</strong>
<ol>
  <li><a href="#s1">Executive Summary</a></li>
  <li><a href="#s2">Current Elixir RLM Codebase</a></li>
  <li><a href="#s3">Original RLM (Python) &mdash; Key Concepts</a></li>
  <li><a href="#s4">Pi Coding Agent &mdash; Architecture Lessons</a></li>
  <li><a href="#s5">Comparative Analysis</a></li>
  <li><a href="#s6">What Works Well (Keep)</a></li>
  <li><a href="#s7">Gaps &amp; Issues (Fix)</a></li>
  <li><a href="#s8">Missing Pieces for a Coding Agent</a></li>
  <li><a href="#s9">OTP Design Patterns at Play</a></li>
  <li><a href="#s10">Tracing &amp; Observability Deep Dive</a></li>
  <li><a href="#s11">Recommended Architecture</a></li>
</ol>
</nav>

<!-- ================================================================ -->
<h2 id="s1">1. Executive Summary</h2>

<div class="card">
<p>You have a <strong>solid RLM engine</strong> in Elixir that correctly implements the core recursive language model pattern using OTP primitives. The async-eval pattern for deadlock prevention is elegant and correct. The telemetry/event system is well-structured.</p>

<p>To evolve this into a <strong>full coding agent</strong> (inspired by Pi), you need to add:</p>
<ul>
  <li><strong>A tool system</strong> &mdash; file read/write/edit, shell execution, grep, glob (replacing the current eval-only approach)</li>
  <li><strong>Anthropic tool_use API support</strong> &mdash; native tool calling instead of code-block extraction</li>
  <li><strong>A session/conversation layer</strong> &mdash; multi-turn interactive sessions with steering</li>
  <li><strong>Rich tracing</strong> &mdash; structured event recording with tree visualization</li>
  <li><strong>A thin interface</strong> &mdash; IEx helpers + simple LiveView dashboard</li>
</ul>
<p>The RLM recursive subcall capability becomes a <strong>superpower</strong> for the coding agent: the ability to spawn sub-agents for complex research, refactoring across many files, or parallel analysis.</p>
</div>

<!-- ================================================================ -->
<h2 id="s2">2. Current Elixir RLM Codebase</h2>

<h3>2.1 Architecture Overview</h3>

<div class="diagram">RLM.Supervisor (one_for_one)
├── Registry (RLM.Registry)          &mdash; named process lookup
├── Phoenix.PubSub (RLM.PubSub)     &mdash; event broadcasting
├── DynamicSupervisor (RLM.WorkerSup)&mdash; temporary Workers
├── DynamicSupervisor (RLM.EventStore)&mdash; per-run EventLog Agents
└── RLM.Telemetry (GenServer)        &mdash; handler attachment</div>

<h3>2.2 The Iterate Loop (Worker GenServer)</h3>

<div class="diagram">┌─────────────────────────────────────────────┐
│  Worker.init/1                              │
│  ├── Build system_msg + user_msg            │
│  ├── Emit [:rlm, :node, :start]            │
│  └── send(self(), :iterate)                 │
│                                             │
│  handle_info(:iterate)                      │
│  ├── maybe_compact(state)                   │
│  ├── LLM.chat(history, model, config)  SYNC │
│  ├── LLM.extract_code(response)            │
│  └── spawn async eval ──┐                  │
│                          │                  │
│  [Worker mailbox FREE]   │                  │
│  ├── handle_call(:spawn_subcall) ◄──────┐   │
│  │   ├── Start child Worker             │   │
│  │   └── Store `from` in pending_subcalls│  │
│  │                                      │   │
│  ├── handle_info(:rlm_result) ◄─ child  │   │
│  │   └── GenServer.reply(from, result)  │   │
│  │                                      │   │
│  └── handle_info(:eval_complete) ◄──────┘   │
│      ├── Update bindings                    │
│      ├── Check final_answer                 │
│      └── send(self(), :iterate) OR complete │
└─────────────────────────────────────────────┘</div>

<p>This async-eval pattern is <strong>the key insight</strong>. The Worker stays responsive for subcall requests while eval runs in a separate process. Without this, <code>lm_query()</code> inside eval'd code would deadlock.</p>

<h3>2.3 Module Inventory</h3>

<table>
<tr><th>Module</th><th>Lines</th><th>Purpose</th><th>Verdict</th></tr>
<tr><td><code>RLM</code></td><td>67</td><td>Public API: <code>run/3</code>, <code>run_async/3</code></td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.Worker</code></td><td>492</td><td>GenServer iterate loop</td><td><span class="badge warn">Refactor</span></td></tr>
<tr><td><code>RLM.Eval</code></td><td>79</td><td>Sandboxed Code.eval_string</td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.Sandbox</code></td><td>55</td><td>Functions in eval scope</td><td><span class="badge warn">Evolve</span></td></tr>
<tr><td><code>RLM.LLM</code></td><td>110</td><td>Anthropic Messages API</td><td><span class="badge warn">Extend</span></td></tr>
<tr><td><code>RLM.Prompt</code></td><td>112</td><td>Message formatting</td><td><span class="badge warn">Refactor</span></td></tr>
<tr><td><code>RLM.Config</code></td><td>70</td><td>Configuration</td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.Helpers</code></td><td>65</td><td>chunks, grep, preview</td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.Truncate</code></td><td>~40</td><td>Head+tail truncation</td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.Span</code></td><td>~15</td><td>ID generation</td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.EventLog</code></td><td>109</td><td>Per-run trace store</td><td><span class="badge warn">Extend</span></td></tr>
<tr><td><code>RLM.Telemetry.*</code></td><td>~150</td><td>3 handlers</td><td><span class="badge good">Keep</span></td></tr>
<tr><td><code>RLM.Application</code></td><td>18</td><td>Supervision tree</td><td><span class="badge good">Keep</span></td></tr>
</table>

<h3>2.4 The Three Invariants</h3>
<div class="card">
<ol>
  <li><strong>Raw data never enters the LLM context window</strong> &mdash; only metadata + 500-char preview. Full data lives in bindings (<code>context</code> variable).</li>
  <li><strong>Sub-LLM outputs stay in variables</strong> &mdash; results of <code>lm_query()</code> are stored, not shown to parent LLM.</li>
  <li><strong>Stdout is truncated</strong> &mdash; head+tail strategy (4000+4000 chars). Important results must be stored in variables.</li>
</ol>
<p>These invariants are elegant for data processing tasks. For a coding agent, invariant #1 needs adaptation &mdash; the agent needs to <em>see</em> file contents (though still with size limits).</p>
</div>

<!-- ================================================================ -->
<h2 id="s3">3. Original RLM (Python) &mdash; Key Concepts</h2>

<div class="card">
<p><strong>Paper:</strong> <a href="https://arxiv.org/abs/2512.24601v1">arxiv.org/abs/2512.24601v1</a> &mdash; <strong>Repo:</strong> <a href="https://github.com/alexzhang13/rlm">github.com/alexzhang13/rlm</a></p>
</div>

<h3>3.1 Core Algorithm</h3>
<p>RLM treats an LLM as a <strong>recursive function</strong> operating in a REPL:</p>
<ol>
  <li>LLM receives task + metadata about context (not raw data)</li>
  <li>LLM writes code that gets executed in a persistent environment</li>
  <li>Code can call <code>rlm_query()</code> to spawn child RLMs (recursive subcalls)</li>
  <li>Each child operates on a strictly smaller/more abstract input (monotonicity)</li>
  <li>Results flow back through variable bindings</li>
  <li>Loop continues until <code>FINAL(answer)</code> is set</li>
</ol>

<h3>3.2 Python Architecture</h3>
<table>
<tr><th>Component</th><th>Python</th><th>Elixir Equivalent</th></tr>
<tr><td>Core loop</td><td><code>RLM_REPL.completion()</code></td><td><code>RLM.Worker.handle_info(:iterate)</code></td></tr>
<tr><td>Code execution</td><td><code>LocalREPL.execute_code()</code> via <code>exec()</code></td><td><code>RLM.Eval.run()</code> via <code>Code.eval_string</code></td></tr>
<tr><td>Subcall communication</td><td>TCP sockets with length-prefixed JSON</td><td>GenServer.call + message passing</td></tr>
<tr><td>Recursion</td><td><code>RLM._subcall()</code> creates child RLM</td><td><code>Worker.handle_call(:spawn_subcall)</code></td></tr>
<tr><td>Budget tracking</td><td>Cost accumulation in <code>_cumulative_cost</code></td><td>Config fields exist but unused</td></tr>
<tr><td>Environment isolation</td><td>Separate processes + socket protocol</td><td>Separate BEAM processes + mailbox</td></tr>
</table>

<h3>3.3 What the Elixir Version Does Better</h3>
<ul>
  <li><span class="badge good">Better</span> <strong>No sockets needed</strong> &mdash; BEAM message passing replaces TCP sockets entirely. This is dramatically simpler and more reliable.</li>
  <li><span class="badge good">Better</span> <strong>OTP supervision</strong> &mdash; crashed workers get cleaned up properly. Python version must manually handle process lifecycle.</li>
  <li><span class="badge good">Better</span> <strong>Registry-based discovery</strong> &mdash; workers found by span_id through Registry, not PID tracking.</li>
  <li><span class="badge good">Better</span> <strong>Telemetry integration</strong> &mdash; structured events with 3 handlers vs. Python's logger-based approach.</li>
  <li><span class="badge good">Better</span> <strong>Parallel subcalls</strong> &mdash; <code>parallel_query</code> uses Task.async_many, naturally leveraging BEAM concurrency.</li>
</ul>

<h3>3.4 What Python Version Has That We Don't</h3>
<ul>
  <li><span class="badge warn">Missing</span> <strong>Cost tracking</strong> &mdash; Python accumulates cost across the recursion tree. Our config has cost fields but never calculates.</li>
  <li><span class="badge warn">Missing</span> <strong>Budget propagation</strong> &mdash; Python passes remaining budget/timeout to children. Our children get full config.</li>
  <li><span class="badge warn">Missing</span> <strong>Compaction via summarization</strong> &mdash; Python uses an LLM call to summarize history. Our compaction just truncates.</li>
  <li><span class="badge warn">Missing</span> <strong>Multiple environment types</strong> &mdash; Python has LocalREPL + DockerREPL. We only have local eval.</li>
</ul>

<!-- ================================================================ -->
<h2 id="s4">4. Pi Coding Agent &mdash; Architecture Lessons</h2>

<div class="card">
<p><strong>Repo:</strong> <a href="https://github.com/badlogic/pi-mono">github.com/badlogic/pi-mono</a></p>
<p>Pi is a monorepo with three layers: <code>pi-ai</code> (LLM abstraction), <code>pi-agent-core</code> (agent loop + tools), and <code>pi-coding-agent</code> (TUI coding agent).</p>
</div>

<h3>4.1 Three-Layer Architecture</h3>
<div class="diagram">┌──────────────────────────────────┐
│  pi-coding-agent (TUI/interface) │
│  ├── system-prompt.ts            │
│  ├── tools/  (bash, edit, read..)│
│  ├── compaction/                 │
│  ├── session-manager.ts          │
│  └── skills.ts                   │
├──────────────────────────────────┤
│  pi-agent-core (agent loop)      │
│  ├── agent-loop.ts               │
│  ├── types.ts (AgentState, etc.) │
│  └── event system                │
├──────────────────────────────────┤
│  pi-ai (LLM abstraction)        │
│  ├── Provider registry           │
│  ├── Streaming support           │
│  ├── Tool argument validation    │
│  └── Multi-provider (Anthropic,  │
│       OpenAI, Google, etc.)      │
└──────────────────────────────────┘</div>

<h3>4.2 Agent Loop &mdash; Key Design</h3>
<p>Pi's agent loop is conceptually simple:</p>
<pre>
while (hasMoreToolCalls || pendingMessages.length > 0) {
    // 1. Process pending steering messages
    // 2. Stream assistant response from LLM
    // 3. Execute tool calls if present
    // 4. Check for user steering interruptions
    // 5. Collect follow-up messages
}
// Check for follow-up messages before exiting
</pre>

<p>Key patterns:</p>
<ul>
  <li><strong>Tool-use native</strong> &mdash; uses Anthropic's <code>tool_use</code> API, not code extraction from markdown</li>
  <li><strong>Streaming throughout</strong> &mdash; responses stream to UI as they arrive</li>
  <li><strong>Steering</strong> &mdash; user can interrupt tool execution mid-flight</li>
  <li><strong>Event-driven</strong> &mdash; granular events: agent_start/end, turn_start/end, message_start/update/end, tool_execution_start/update/end</li>
  <li><strong>Context transformation</strong> &mdash; <code>transformContext()</code> prunes messages before LLM call</li>
</ul>

<h3>4.3 Pi's Tool System</h3>
<table>
<tr><th>Tool</th><th>Description</th><th>Elixir Equivalent Needed</th></tr>
<tr><td><code>bash</code></td><td>Shell command execution</td><td><code>Tool.Bash</code></td></tr>
<tr><td><code>read</code></td><td>File reading with line ranges</td><td><code>Tool.Read</code></td></tr>
<tr><td><code>write</code></td><td>File creation</td><td><code>Tool.Write</code></td></tr>
<tr><td><code>edit</code></td><td>Exact string replacement in files</td><td><code>Tool.Edit</code></td></tr>
<tr><td><code>edit-diff</code></td><td>Diff-based editing</td><td>(optional, start with edit)</td></tr>
<tr><td><code>grep</code></td><td>Content search</td><td><code>Tool.Grep</code></td></tr>
<tr><td><code>find</code></td><td>File discovery by pattern</td><td><code>Tool.Glob</code></td></tr>
<tr><td><code>ls</code></td><td>Directory listing</td><td><code>Tool.Ls</code></td></tr>
</table>

<h3>4.4 What to Take from Pi</h3>
<div class="grid">
<div class="card">
<h3>Adopt</h3>
<ul>
  <li>Tool-use API (not code block extraction) for the coding agent mode</li>
  <li>Granular event system for live UI updates</li>
  <li>Streaming responses</li>
  <li>System prompt as a composable template</li>
  <li>Session management with persistence</li>
  <li>Context compaction strategy</li>
</ul>
</div>
<div class="card">
<h3>Simplify/Skip</h3>
<ul>
  <li>Multi-provider support (start Claude-only)</li>
  <li>TUI (use IEx + LiveView)</li>
  <li>Complex keybinding system</li>
  <li>OAuth/auth storage</li>
  <li>Package manager integration</li>
  <li>Slash commands (initially)</li>
</ul>
</div>
</div>

<!-- ================================================================ -->
<h2 id="s5">5. Comparative Analysis</h2>

<table>
<tr><th>Aspect</th><th>Your Elixir RLM</th><th>Original RLM (Python)</th><th>Pi Coding Agent</th></tr>
<tr><td>Execution model</td><td>Code eval in REPL</td><td>Code eval in REPL</td><td>Tool use API</td></tr>
<tr><td>Recursion</td><td>Yes, via subcalls</td><td>Yes, via subcalls</td><td>No native recursion</td></tr>
<tr><td>Concurrency</td><td>BEAM processes</td><td>TCP sockets + multiprocessing</td><td>Sequential tool calls</td></tr>
<tr><td>Supervision</td><td>OTP DynamicSupervisor</td><td>Manual process management</td><td>None (single-threaded JS)</td></tr>
<tr><td>Streaming</td><td>No</td><td>No</td><td>Yes, full streaming</td></tr>
<tr><td>Tool system</td><td>Sandbox functions only</td><td>Sandbox functions only</td><td>Rich tool registry</td></tr>
<tr><td>Context mgmt</td><td>Truncation + compaction</td><td>LLM summarization</td><td>Custom transform pipeline</td></tr>
<tr><td>Tracing</td><td>Telemetry + EventLog + PubSub</td><td>Logger only</td><td>Event stream + UI rendering</td></tr>
<tr><td>Interface</td><td>IEx / programmatic only</td><td>CLI</td><td>Rich TUI</td></tr>
<tr><td>Testing</td><td>MockLLM + deterministic</td><td>Manual testing</td><td>N/A in public repo</td></tr>
</table>

<p><strong>Key insight:</strong> Your project uniquely combines RLM's recursive capability with OTP's native concurrency. Pi has no recursion; original RLM has no tool system. You can have both.</p>

<!-- ================================================================ -->
<h2 id="s6">6. What Works Well (Keep)</h2>

<div class="grid">
<div class="card">
<h3>Async Eval Pattern</h3>
<p>The deadlock-prevention design where eval runs in a separate process while the Worker remains responsive is <strong>correct and elegant</strong>. This is the foundation everything else builds on.</p>
</div>
<div class="card">
<h3>OTP Supervision Tree</h3>
<p>Registry-based lookup, DynamicSupervisor for temporary workers, <code>restart: :temporary</code> &mdash; all idiomatic OTP. The tree is simple and correct.</p>
</div>
<div class="card">
<h3>Telemetry Architecture</h3>
<p>14 event types with 3 independent handlers (Logger, EventLog, PubSub) is a clean separation. PubSub broadcasts enable LiveView without coupling.</p>
</div>
<div class="card">
<h3>Test Infrastructure</h3>
<p>MockLLM with ETS-based response queue, dependency injection via <code>llm_module</code> config, and clear test categories. Solid foundation for CI.</p>
</div>
</div>

<!-- ================================================================ -->
<h2 id="s7">7. Gaps &amp; Issues (Fix)</h2>

<h3>7.1 Critical</h3>

<div class="card">
<p><span class="badge err">Critical</span> <strong>No max_concurrent_subcalls enforcement</strong></p>
<p>Config has <code>max_concurrent_subcalls: 10</code> but <code>handle_call(:spawn_subcall)</code> never checks it. A runaway <code>parallel_query</code> with 1000 items would spawn 1000 child workers simultaneously.</p>
<p><strong>Fix:</strong> Check <code>map_size(state.pending_subcalls) >= config.max_concurrent_subcalls</code> before spawning.</p>
</div>

<div class="card">
<p><span class="badge err">Critical</span> <strong>No cancellation/timeout for root RLM.run</strong></p>
<p><code>RLM.run/3</code> blocks forever with <code>receive</code>. If the worker crashes without sending a result (e.g., linked process crash), the caller hangs.</p>
<p><strong>Fix:</strong> Use <code>receive ... after timeout -></code> or <code>Process.monitor</code> on the worker.</p>
</div>

<h3>7.2 Important</h3>

<div class="card">
<p><span class="badge warn">Important</span> <strong>EventLog agents never cleaned up</strong></p>
<p>Per-run Agents persist indefinitely in the EventStore DynamicSupervisor. Over many runs, this leaks memory.</p>
<p><strong>Fix:</strong> Add TTL-based cleanup or bounded cache. Could use a periodic GenServer that sweeps old entries.</p>
</div>

<div class="card">
<p><span class="badge warn">Important</span> <strong>Cost tracking fields exist but are never calculated</strong></p>
<p>Config has <code>cost_per_1k_prompt_tokens_*</code> fields that are set but never used in any calculation. The original Python RLM tracks cumulative cost including children.</p>
<p><strong>Fix:</strong> Accumulate cost in Worker state, emit in telemetry, propagate through recursion tree.</p>
</div>

<div class="card">
<p><span class="badge warn">Important</span> <strong>No streaming support</strong></p>
<p><code>RLM.LLM.chat/3</code> waits for the complete response. For a coding agent with interactive use, streaming is essential for responsiveness.</p>
<p><strong>Fix:</strong> Implement SSE streaming with the Anthropic API. This also enables token-by-token PubSub broadcast.</p>
</div>

<div class="card">
<p><span class="badge warn">Important</span> <strong>History compaction is lossy</strong></p>
<p>At 80% context window, the entire history (except system message) is serialized to a string and stored in <code>compacted_history</code>. The LLM only sees a truncated preview. Fine details of reasoning are lost.</p>
<p><strong>Consider:</strong> Use an LLM call to summarize key decisions and state (like original RLM) or implement sliding-window compaction that preserves recent turns.</p>
</div>

<h3>7.3 Minor</h3>
<ul>
  <li><span class="badge warn">Minor</span> <code>parallel_query</code> lacks error aggregation &mdash; if one task fails, others may still run (resource leak)</li>
  <li><span class="badge warn">Minor</span> Eval timeout uses <code>Process.exit(pid, :kill)</code> without trying graceful shutdown first</li>
  <li><span class="badge warn">Minor</span> Loop detection (Jaccard similarity) is simplistic &mdash; catches word-level repetition but not semantic loops</li>
  <li><span class="badge warn">Minor</span> <code>RLM.run/3</code> doesn't return <code>run_id</code> for trace retrieval after sync execution</li>
</ul>

<!-- ================================================================ -->
<h2 id="s8">8. Missing Pieces for a Coding Agent</h2>

<p>The current codebase is an <strong>RLM engine</strong>. To become a <strong>coding agent</strong>, it needs these additional capabilities:</p>

<table>
<tr><th>Capability</th><th>Priority</th><th>Notes</th></tr>
<tr><td>Tool system (read, write, edit, bash, grep, glob)</td><td><span class="badge err">P0</span></td><td>Core of any coding agent. Use Anthropic tool_use API.</td></tr>
<tr><td>Anthropic tool_use API support</td><td><span class="badge err">P0</span></td><td>Replace code-block extraction with native tool calling for the agent mode.</td></tr>
<tr><td>Streaming responses</td><td><span class="badge err">P0</span></td><td>Essential for interactive use. SSE from Anthropic API.</td></tr>
<tr><td>Session/conversation management</td><td><span class="badge warn">P1</span></td><td>Multi-turn conversations with history, persistence.</td></tr>
<tr><td>System prompt for coding</td><td><span class="badge warn">P1</span></td><td>Coding-specific instructions, project context injection.</td></tr>
<tr><td>IEx helper module</td><td><span class="badge warn">P1</span></td><td><code>Agent.chat("fix the tests")</code> from IEx.</td></tr>
<tr><td>LiveView dashboard</td><td><span class="badge warn">P1</span></td><td>Real-time session view, trace visualization.</td></tr>
<tr><td>Context compaction (smarter)</td><td><span class="badge warn">P1</span></td><td>Sliding window or LLM summarization.</td></tr>
<tr><td>Cost tracking &amp; budgets</td><td><span class="badge good">P2</span></td><td>Track spend per session, set limits.</td></tr>
<tr><td>Session persistence (disk)</td><td><span class="badge good">P2</span></td><td>Resume sessions across restarts.</td></tr>
<tr><td>Permission system for tools</td><td><span class="badge good">P2</span></td><td>Auto-allow reads, confirm writes/bash.</td></tr>
<tr><td>RLM mode toggle</td><td><span class="badge good">P2</span></td><td>Enable recursive subcalls when the agent decides to use them.</td></tr>
</table>

<!-- ================================================================ -->
<h2 id="s9">9. OTP Design Patterns at Play</h2>

<p>The beauty of building this on BEAM/OTP is how naturally the patterns map:</p>

<table>
<tr><th>Concept</th><th>OTP Pattern</th><th>Why It Fits</th></tr>
<tr><td>Agent session</td><td>GenServer</td><td>Stateful conversation with message history, tool state</td></tr>
<tr><td>Recursive subcalls</td><td>DynamicSupervisor + GenServer</td><td>Each child is a supervised, temporary process with its own state</td></tr>
<tr><td>Tool execution</td><td>Task (under TaskSupervisor)</td><td>Short-lived, supervised, result-awaited</td></tr>
<tr><td>Event broadcasting</td><td>Phoenix.PubSub</td><td>Decouple producer (agent) from consumers (LiveView, logger)</td></tr>
<tr><td>Process discovery</td><td>Registry</td><td>Find agents/sessions by name without ETS</td></tr>
<tr><td>Streaming</td><td>GenStage or simple message passing</td><td>Backpressure-aware token streaming</td></tr>
<tr><td>Session persistence</td><td>Agent or ETS + periodic flush</td><td>In-memory with disk backup</td></tr>
<tr><td>Trace recording</td><td>Agent (append-only store)</td><td>Already implemented in EventLog</td></tr>
<tr><td>Crash isolation</td><td><code>restart: :temporary</code></td><td>One bad tool call doesn't take down the session</td></tr>
</table>

<p>The pattern that's <strong>uniquely powerful</strong> on BEAM: an agent can spawn sub-agents as supervised processes that run concurrently, each with their own LLM context and tool access, while the parent stays responsive. No threads, no sockets, no orchestration framework needed.</p>

<!-- ================================================================ -->
<h2 id="s10">10. Tracing &amp; Observability Deep Dive</h2>

<p>You emphasized tracing is very important. Here's the current state and what to build:</p>

<h3>10.1 Current Tracing Infrastructure</h3>
<div class="card">
<p><strong>What exists (good foundation):</strong></p>
<ul>
  <li>14 telemetry events covering the full lifecycle</li>
  <li><code>EventLog</code> &mdash; per-run Agent storing structured events, builds a span tree</li>
  <li><code>PubSub</code> handler &mdash; broadcasts all events for real-time consumption</li>
  <li><code>Logger</code> handler &mdash; structured logging</li>
  <li>JSONL export from EventLog</li>
  <li>Span/run ID system for correlating events across the recursion tree</li>
</ul>
</div>

<h3>10.2 What to Add for the Coding Agent</h3>

<div class="grid">
<div class="card">
<h3>Recording</h3>
<ul>
  <li>Tool execution events (tool_start, tool_end with args/result)</li>
  <li>Token-level streaming events</li>
  <li>Session-level events (session_start, session_end)</li>
  <li>Full message history snapshots at key points</li>
  <li>Cost accumulation per turn/session</li>
  <li>Git state before/after tool execution</li>
  <li>Disk persistence for sessions (not just in-memory Agent)</li>
</ul>
</div>
<div class="card">
<h3>Visualization (LiveView)</h3>
<ul>
  <li>Real-time token streaming display</li>
  <li>Tool call timeline (what was called, when, result)</li>
  <li>Recursion tree visualization (span hierarchy)</li>
  <li>Token usage graphs per turn</li>
  <li>Session cost tracker</li>
  <li>Diff viewer for file modifications</li>
  <li>Event log replay (step through execution)</li>
</ul>
</div>
</div>

<h3>10.3 Recommended Telemetry Events for Coding Agent</h3>
<pre>
# Existing (keep all)
[:rlm, :node, :start | :stop]
[:rlm, :iteration, :start | :stop]
[:rlm, :llm, :request, :start | :stop | :exception]
[:rlm, :eval, :start | :stop]
[:rlm, :subcall, :spawn | :result]
[:rlm, :compaction, :run]

# New for coding agent
[:agent, :session, :start | :stop]
[:agent, :turn, :start | :stop]
[:agent, :message, :start | :delta | :stop]   # streaming
[:agent, :tool, :start | :stop | :error]       # tool execution
[:agent, :tool, :permission, :requested | :granted | :denied]
[:agent, :cost, :update]                       # running cost
[:agent, :context, :compaction]                # smarter compaction
</pre>

<!-- ================================================================ -->
<h2 id="s11">11. Recommended Architecture</h2>

<h3>11.1 Dual-Mode Design</h3>
<p>The key architectural insight: keep the <strong>RLM engine</strong> as-is (it works well for data processing) and build the <strong>coding agent</strong> as a separate mode that can optionally use RLM capabilities:</p>

<div class="diagram">┌─────────────────────────────────────────────────┐
│  Interface Layer                                │
│  ├── IEx Helpers  (Agent.chat/1, Agent.status/0)│
│  └── LiveView Dashboard (sessions, traces)      │
├─────────────────────────────────────────────────┤
│  Agent Layer (NEW)                              │
│  ├── Agent.Session (GenServer)                  │
│  │   ├── Conversation state                     │
│  │   ├── Tool registry                          │
│  │   ├── System prompt composition              │
│  │   └── Turn loop (stream &#8594; tools &#8594; repeat)   │
│  ├── Agent.Tool (behaviour)                     │
│  │   ├── Tool.Bash                              │
│  │   ├── Tool.Read / Tool.Write / Tool.Edit     │
│  │   ├── Tool.Grep / Tool.Glob / Tool.Ls       │
│  │   └── Tool.RLM (bridges to RLM engine!)      │
│  ├── Agent.LLM (extends RLM.LLM)               │
│  │   ├── tool_use support                       │
│  │   └── streaming (SSE)                        │
│  └── Agent.Prompt                               │
│      └── System prompt templates                │
├─────────────────────────────────────────────────┤
│  RLM Engine (EXISTING - enhanced)               │
│  ├── RLM.Worker (iterate loop)                  │
│  ├── RLM.Eval (sandboxed code execution)        │
│  ├── RLM.Sandbox (REPL functions)               │
│  └── RLM.LLM / RLM.Config / RLM.Helpers        │
├─────────────────────────────────────────────────┤
│  Core Infrastructure (EXISTING)                 │
│  ├── Telemetry (14+ events, 3+ handlers)        │
│  ├── EventLog (per-run structured trace)        │
│  ├── Registry + PubSub                          │
│  └── Supervision tree                           │
└─────────────────────────────────────────────────┘</div>

<h3>11.2 The Bridge: Tool.RLM</h3>
<p>This is where the magic happens. The coding agent has a tool called <code>rlm</code> that lets the LLM invoke the full RLM engine for complex data processing tasks. The agent can decide when a task is complex enough to warrant recursive processing:</p>

<pre>
# Agent decides to use RLM for a complex analysis task
tool_use: rlm
input: {
  "context": "(contents of large log file)",
  "query": "Find all error patterns and categorize them",
  "model_size": "small"
}
# This spawns an RLM.Worker which can itself recursively subcall
</pre>

<h3>11.3 Supervision Tree (Extended)</h3>
<div class="diagram">RLM.Supervisor (one_for_one)
├── Registry (RLM.Registry)
├── Phoenix.PubSub (RLM.PubSub)
├── DynamicSupervisor (RLM.WorkerSup)     &mdash; RLM workers
├── DynamicSupervisor (RLM.EventStore)     &mdash; event log agents
├── DynamicSupervisor (Agent.SessionSup)   &mdash; agent sessions (NEW)
├── Task.Supervisor (Agent.ToolSup)        &mdash; tool execution (NEW)
├── RLM.Telemetry
└── Agent.SessionStore (GenServer)         &mdash; session persistence (NEW)</div>

<hr>

<div class="card" style="border-color: var(--accent);">
<h3>Bottom Line</h3>
<p>You have a <strong>working, well-tested RLM engine</strong> that correctly uses OTP primitives. The path to a coding agent is:</p>
<ol>
  <li>Add tool_use API support + streaming to the LLM client</li>
  <li>Build the tool system (behaviour + implementations)</li>
  <li>Create the Agent.Session GenServer (tool-use loop, not eval loop)</li>
  <li>Bridge to RLM engine via Tool.RLM</li>
  <li>Add IEx helpers + LiveView for interface</li>
  <li>Enhance tracing for both recording and visualization</li>
</ol>
<p>The RLM engine doesn't need to be rewritten &mdash; it becomes a <strong>capability</strong> that the coding agent can invoke when needed. Two modes, one codebase, shared OTP infrastructure.</p>
</div>

</body>
</html>
